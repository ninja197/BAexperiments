{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniLM_NER",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17VmOQtajQ9sXLeflKKv0MbU1ivWKRYRc",
      "authorship_tag": "ABX9TyNeBk7r+bjmUsDWJiCpOw7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja197/BAexperiments/blob/main/MiniLM_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxzDmOlPNUfA",
        "outputId": "156178ec-c5c1-4824-edb2-4416b020468c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 9.4 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 65.1 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=a249a1aa9edda5537ed14e90e9c92f0b72cb37766fd290a4bddbfe5b43a6fe68\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, tokenizers, seqeval, sentencepiece, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 sentencepiece-0.1.96 seqeval-1.2.2 tokenizers-0.12.1 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-fo8u1pr2\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-fo8u1pr2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.12.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=4036494 sha256=2a127e59499fb99c9afcc74fed2d5dac94e6802610695dc9e84cc2ac95d262bf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-de0pzirz/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.49 transformers-4.19.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets tokenizers seqeval sentencepiece\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tokenizers import SentencePieceUnigramTokenizer\n",
        "from transformers import AutoTokenizer, XLMRobertaTokenizer\n",
        "from transformers import AutoModelForTokenClassification, AutoModelForMaskedLM\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification, DataCollatorForLanguageModeling\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "BIcN2H7iNoMJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDP0Rk1CNoZN",
        "outputId": "8d207e8e-9885-4154-cee8-5102f8b90b9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'nds'"
      ],
      "metadata": {
        "id": "GCCutF64IFLa"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load standard tokenizer\n",
        "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmPlW24VNok8",
        "outputId": "291a318b-5165-4272-a9a4-ec53effa57fc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning"
      ],
      "metadata": {
        "id": "StSOz_YPOegN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file, lang, max_len, tokenizer, assignment):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.assignment = assignment\n",
        "        self.lang = lang\n",
        "\n",
        "        self.create_label2id()\n",
        "\n",
        "        self.examples = self.read_file(file)\n",
        "\n",
        "        print(self.examples[0])\n",
        "        print('----------------------------------------')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encode(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def create_label2id(self):\n",
        "\n",
        "        ner_tags = [\n",
        "            'B-ORG',\n",
        "            'I-ORG',\n",
        "            'B-PER',\n",
        "            'I-PER',\n",
        "            'B-MISC',\n",
        "            'I-MISC',\n",
        "            'B-LOC',\n",
        "            'I-LOC',\n",
        "            'O'\n",
        "        ]\n",
        "\n",
        "        iter = 0\n",
        "        self.label2id = {}\n",
        "        for tag in ner_tags:\n",
        "            self.label2id[tag] = iter\n",
        "            iter += 1\n",
        "\n",
        "    def read_file(self, file, convert_labels=True):\n",
        "\n",
        "        inps = []\n",
        "\n",
        "        with open(file, 'r') as f:\n",
        "            temp_tokens = []\n",
        "            temp_labels = []\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "\n",
        "                    token = line.strip().split('\\t')\n",
        "                    assert len(token) == 2\n",
        "\n",
        "                    if convert_labels:\n",
        "                        temp_tokens.append(token[0].replace(self.lang + ':', ''))\n",
        "                        temp_labels.append(self.label2id[token[1]])\n",
        "\n",
        "                    else:\n",
        "                        temp_tokens.append(token[0].replace(self.lang + ':', ''))\n",
        "                        temp_labels.append(token[1])\n",
        "\n",
        "                else:\n",
        "                    inps.append((temp_tokens,temp_labels))\n",
        "                    temp_tokens = []\n",
        "                    temp_labels = []\n",
        "        return inps\n",
        "\n",
        "    def encode(self, id):\n",
        "        instance = self.examples[id]\n",
        "\n",
        "\n",
        "        forms = instance[0]\n",
        "        labels = instance[1]\n",
        "\n",
        "        expanded_labels = []\n",
        "        label_mask = []\n",
        "\n",
        "        for i in range(0, len(forms)):\n",
        "\n",
        "            subwords = self.tokenizer.tokenize(forms[i])\n",
        "\n",
        "            if self.assignment == 'first':\n",
        "                expanded_labels.append(labels[i])\n",
        "                for j in range(1, len(subwords)):\n",
        "                    expanded_labels.append(-100)\n",
        "            elif self.assignment == 'all':\n",
        "                for j in range(0,len(subwords)):\n",
        "                    expanded_labels.append(labels[i])\n",
        "                    if j < len(subwords) - 1:\n",
        "                        label_mask.append(0)\n",
        "                    else:\n",
        "                        label_mask.append(1)\n",
        "\n",
        "            elif self.assignment == 'last':\n",
        "                for j in range(0,len(subwords)-1):\n",
        "                    expanded_labels.append(-100)\n",
        "                expanded_labels.append(labels[i])\n",
        "\n",
        "\n",
        "        s1 = ' '.join(forms)\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            s1,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        if len(expanded_labels) > self.max_len:\n",
        "            expanded_labels = expanded_labels[:self.max_len]\n",
        "\n",
        "        enc['labels'] = expanded_labels\n",
        "\n",
        "        return enc\n"
      ],
      "metadata": {
        "id": "Kqp8Ls7fOdfE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filefinder\n",
        "def biblelang2nerlang(bible_lang):\n",
        "    language_mapping = '/content/drive/MyDrive/NER/bible_ner_xlmr_split.txt'\n",
        "    with open(language_mapping, 'r') as f:\n",
        "        for line in f:\n",
        "            data = line.strip().split(',')\n",
        "            if data[1] == bible_lang:\n",
        "                return data[2]\n",
        "\n",
        "def lang_to_ner(lang, split):\n",
        "\n",
        "    ner_dir = '/content/drive/MyDrive/{lang}/{split}'.format(lang=biblelang2nerlang(lang), split=split)\n",
        "\n",
        "    return ner_dir"
      ],
      "metadata": {
        "id": "eEayrOuZO6mh"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training (english) and evaluation dataset (target language)\n",
        "ner_train_dataset = NERDataset(file=lang_to_ner('eng', 'train'),\n",
        "                                       lang='en', max_len=256, tokenizer=xlmr_tokenizer,\n",
        "                                       assignment='last')\n",
        "\n",
        "\n",
        "ner_eval_dataset = NERDataset(file=lang_to_ner(language, 'dev'),\n",
        "                          lang=biblelang2nerlang(language), max_len=256, tokenizer=xlmr_tokenizer,\n",
        "                          assignment='last')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDmPWu5RPDce",
        "outputId": "21f48933-84cf-4303-910e-0b866ff1fccf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['R.H.', 'Saunders', '(', 'St.', 'Lawrence', 'River', ')', '(', '968', 'MW', ')'], [0, 1, 8, 0, 1, 1, 8, 8, 8, 8, 8])\n",
            "----------------------------------------\n",
            "(['Theoderik', 'I', '.', '(', '418-451', ')', ',', 'westgootsch', 'König', ','], [2, 3, 3, 8, 8, 8, 8, 8, 8, 8])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_test_dataset = NERDataset(file=lang_to_ner(language, 'test'), lang=biblelang2nerlang(language), max_len=256, tokenizer=xlmr_tokenizer, assignment='last')\n",
        "\n",
        "print('NER Train set length: {}'.format(ner_train_dataset.__len__()))\n",
        "print('NER Eval set length: {}'.format(ner_eval_dataset.__len__()))\n",
        "print('NER Test set length: {}'.format(ner_test_dataset.__len__()))\n",
        "\n",
        "ner_warmup_steps = int((5 * (ner_train_dataset.__len__() // (32 * 4 * 1))) * .01)\n",
        "\n",
        "!mkdir finetuned_ner_model\n",
        "ner_training_args = TrainingArguments(\n",
        "    output_dir= 'finetuned_ner_model',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    logging_steps=25,\n",
        "    save_total_limit=3,\n",
        "    save_steps=3000,\n",
        "    evaluation_strategy='epoch',\n",
        "    eval_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=ner_warmup_steps,\n",
        "    disable_tqdm=False,\n",
        "    gradient_accumulation_steps=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNAboU4sPDqi",
        "outputId": "1d1449bf-02ce-41f8-f09e-6f456e570414"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['He', 'bruukt', 'as', 'Eerste', 'dat', 'Woort', 'Chemie', '.'], [8, 8, 8, 8, 8, 8, 6, 8])\n",
            "----------------------------------------\n",
            "NER Train set length: 20000\n",
            "NER Eval set length: 100\n",
            "NER Test set length: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import AutoModel\n",
        "\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained('microsoft/Multilingual-MiniLM-L12-H384',num_labels=len(ner_train_dataset.label2id))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SClk30jWPS7K",
        "outputId": "de76a7ae-6f03-446e-f219-e66b5009055c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/12a5ad52cb7fc5542e16e354fe6eb487f2f87edac63bf85dc238b1236dbaf24c.ccf88548169a21266c411bcf65585ba761d762a9c85fde572f529806fdd94ee2\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"microsoft/Multilingual-MiniLM-L12-H384\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 384,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250037\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e1243df19ff0e1975c063fc4c531ba7fdad1ee538e0d94c41a766febfee0c8ab.3d27c3b243133a56a858d62deffdc59141c45422837cf3fde167b873bad92273\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/Multilingual-MiniLM-L12-H384 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from seqeval.metrics import f1_score as seqeval_f1\n",
        "from seqeval.metrics import accuracy_score as seqeval_accuracy\n",
        "\n",
        "\n",
        "def create_id2label_ner():\n",
        "\n",
        "    ner_tags = [\n",
        "        'B-ORG',\n",
        "        'I-ORG',\n",
        "        'B-PER',\n",
        "        'I-PER',\n",
        "        'B-MISC',\n",
        "        'I-MISC',\n",
        "        'B-LOC',\n",
        "        'I-LOC',\n",
        "        'O'\n",
        "    ]\n",
        "\n",
        "    iter = 0\n",
        "    id2label = {}\n",
        "    for tag in ner_tags:\n",
        "        id2label[iter] = tag\n",
        "        iter += 1\n",
        "\n",
        "    return id2label\n",
        "\n",
        "\n",
        "\n",
        "def ner_metrics(eval_pred):\n",
        "\n",
        "    labels = eval_pred.label_ids\n",
        "    preds = eval_pred.predictions.argmax(-1)\n",
        "\n",
        "    corrected_preds = []\n",
        "    corrected_labels = []\n",
        "\n",
        "    id2label = create_id2label_ner()\n",
        "\n",
        "    for i in range(0, len(labels)):\n",
        "        temp_pred = []\n",
        "        temp_label = []\n",
        "        for j in range(0, len(labels[i])):\n",
        "            if labels[i][j] != -100:\n",
        "                temp_label.append(id2label[labels[i][j]])\n",
        "                temp_pred.append(id2label[preds[i][j]])\n",
        "\n",
        "        corrected_labels.append(temp_label)\n",
        "        corrected_preds.append(temp_pred)\n",
        "\n",
        "    acc = seqeval_accuracy(corrected_labels, corrected_preds)\n",
        "    f1 = seqeval_f1(corrected_labels, corrected_preds)\n",
        "\n",
        "    f1 = f1 * 100\n",
        "    acc = acc * 100\n",
        "\n",
        "    print('F1 during training: {}'.format(f1))\n",
        "    print('Accuracy during training: {}'.format(acc))\n",
        "    print('---------------------------------------------')\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "45c9MAJNPekT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=xlmr_tokenizer,\n",
        "    padding='longest'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=ner_model,\n",
        "    data_collator=ner_collator,\n",
        "    args=ner_training_args,\n",
        "    train_dataset=ner_train_dataset,\n",
        "    eval_dataset=ner_eval_dataset,\n",
        "    compute_metrics=ner_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.predict(ner_test_dataset)\n",
        "results = results.metrics        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s5UW2xIiXKYG",
        "outputId": "0df6747e-a3b1-488c-ff17-350601801bc7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 20000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 06:47, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.716000</td>\n",
              "      <td>0.818928</td>\n",
              "      <td>80.726539</td>\n",
              "      <td>50.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.526500</td>\n",
              "      <td>0.645986</td>\n",
              "      <td>83.652876</td>\n",
              "      <td>56.160458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.425400</td>\n",
              "      <td>0.555506</td>\n",
              "      <td>84.863774</td>\n",
              "      <td>57.485030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.354100</td>\n",
              "      <td>0.569292</td>\n",
              "      <td>82.341070</td>\n",
              "      <td>53.254438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.350900</td>\n",
              "      <td>0.538471</td>\n",
              "      <td>83.148335</td>\n",
              "      <td>55.855856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 50.85714285714287\n",
            "Accuracy during training: 80.72653884964683\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 56.16045845272206\n",
            "Accuracy during training: 83.65287588294652\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 57.48502994011977\n",
            "Accuracy during training: 84.86377396569122\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 53.25443786982248\n",
            "Accuracy during training: 82.34106962663977\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to finetuned_ner_model/checkpoint-3000\n",
            "Configuration saved in finetuned_ner_model/checkpoint-3000/config.json\n",
            "Model weights saved in finetuned_ner_model/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 55.85585585585585\n",
            "Accuracy during training: 83.14833501513623\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 51.71339563862928\n",
            "Accuracy during training: 80.76923076923077\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWGMep5NXKzq",
        "outputId": "fe3d52de-8654-42e9-8ce4-d3455b1e8343"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_accuracy': 80.76923076923077,\n",
              " 'test_f1': 51.71339563862928,\n",
              " 'test_loss': 0.6432451605796814,\n",
              " 'test_runtime': 0.2564,\n",
              " 'test_samples_per_second': 390.056,\n",
              " 'test_steps_per_second': 50.707}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}