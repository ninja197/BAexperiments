{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja197/BAexperiments/blob/main/MiniLM_extend_ner_diffFinetuneLang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko69jJHHLu8l",
        "outputId": "bd9af6e1-3665-461f-a583-9a10a36dbc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.6 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.3 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=87f502f640cf9a5b4a3e66895f750c40e6cc232d0327db01ace9fe325d67eb0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, tokenizers, seqeval, sentencepiece, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 sentencepiece-0.1.96 seqeval-1.2.2 tokenizers-0.12.1 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-tdk40tzx\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-tdk40tzx\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=4030018 sha256=989586793e07ae557b2639bec59fdc7b9acd475ddb2fc49249fc1370206a25ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0xrqwcbe/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.49 transformers-4.19.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets tokenizers seqeval sentencepiece\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dUUD-nmSM50Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tokenizers import SentencePieceUnigramTokenizer\n",
        "from transformers import AutoTokenizer, XLMRobertaTokenizer\n",
        "from transformers import AutoModelForTokenClassification, AutoModelForMaskedLM\n",
        "import copy\n",
        "import os\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification, DataCollatorForLanguageModeling\n",
        "import sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkLcDUVC8osn",
        "outputId": "8d5b1ea3-8739-427f-b45c-57b8335dc0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/sp_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d6iuptpLQEkQ"
      },
      "outputs": [],
      "source": [
        "import sentencepiece_model_pb2 as sp_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'arz'\n",
        "finetune_lang = 'ar'"
      ],
      "metadata": {
        "id": "1kqBLZVt3cvc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DObxGfxOrjnZ"
      },
      "source": [
        "# Load extended tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load extended SPM as tokenizer\n",
        "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "xlmr_tokenizer.vocab_file = '/content/drive/MyDrive/{lang}_model/extended_spm.model'.format(lang=language)     \n",
        "xlmr_tokenizer.sp_model.load(xlmr_tokenizer.vocab_file)\n",
        "\n",
        "#Re-align mask token\n",
        "xlmr_tokenizer.fairseq_tokens_to_ids['<mask>'] = xlmr_tokenizer._convert_token_to_id('DUMMY_MASK')"
      ],
      "metadata": {
        "id": "jWb_CIX1V2wb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vp_89b_rAjo"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QF-rQRE0fwhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86df23a-19ee-493d-881a-e26797261712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('(', 'O'), ('أقاليم', 'B-LOC'), ('ما', 'I-LOC'), ('وراء', 'I-LOC'), ('البحار', 'I-LOC'), (')', 'O')]\n",
            "20000\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "\n",
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file, lang, max_len, tokenizer, assignment):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.assignment = assignment\n",
        "        self.lang = lang\n",
        "\n",
        "        self.create_label2id()\n",
        "\n",
        "        self.examples = self.read_file(file)\n",
        "\n",
        "        print(self.examples[0])\n",
        "        print('----------------------------------------')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encode(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def create_label2id(self):\n",
        "\n",
        "        ner_tags = [\n",
        "            'B-ORG',\n",
        "            'I-ORG',\n",
        "            'B-PER',\n",
        "            'I-PER',\n",
        "            'B-MISC',\n",
        "            'I-MISC',\n",
        "            'B-LOC',\n",
        "            'I-LOC',\n",
        "            'O'\n",
        "        ]\n",
        "\n",
        "        iter = 0\n",
        "        self.label2id = {}\n",
        "        for tag in ner_tags:\n",
        "            self.label2id[tag] = iter\n",
        "            iter += 1\n",
        "\n",
        "    def read_file(self, file, convert_labels=True):\n",
        "\n",
        "        inps = []\n",
        "\n",
        "        with open(file, 'r') as f:\n",
        "            temp_tokens = []\n",
        "            temp_labels = []\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "\n",
        "                    token = line.strip().split('\\t')\n",
        "                    assert len(token) == 2\n",
        "\n",
        "                    if convert_labels:\n",
        "                        temp_tokens.append(token[0].replace(self.lang + ':', ''))\n",
        "                        temp_labels.append(self.label2id[token[1]])\n",
        "\n",
        "                    else:\n",
        "                        temp_tokens.append(token[0].replace(self.lang + ':', ''))\n",
        "                        temp_labels.append(token[1])\n",
        "\n",
        "                else:\n",
        "                    inps.append((temp_tokens,temp_labels))\n",
        "                    temp_tokens = []\n",
        "                    temp_labels = []\n",
        "        return inps\n",
        "\n",
        "    def encode(self, id):\n",
        "        instance = self.examples[id]\n",
        "\n",
        "\n",
        "        forms = instance[0]\n",
        "        labels = instance[1]\n",
        "\n",
        "        expanded_labels = []\n",
        "        label_mask = []\n",
        "\n",
        "        for i in range(0, len(forms)):\n",
        "\n",
        "            subwords = self.tokenizer.tokenize(forms[i])\n",
        "\n",
        "            if self.assignment == 'first':\n",
        "                expanded_labels.append(labels[i])\n",
        "                for j in range(1, len(subwords)):\n",
        "                    expanded_labels.append(-100)\n",
        "            elif self.assignment == 'all':\n",
        "                for j in range(0,len(subwords)):\n",
        "                    expanded_labels.append(labels[i])\n",
        "                    if j < len(subwords) - 1:\n",
        "                        label_mask.append(0)\n",
        "                    else:\n",
        "                        label_mask.append(1)\n",
        "\n",
        "            elif self.assignment == 'last':\n",
        "                for j in range(0,len(subwords)-1):\n",
        "                    expanded_labels.append(-100)\n",
        "                expanded_labels.append(labels[i])\n",
        "\n",
        "\n",
        "        s1 = ' '.join(forms)\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            s1,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        if len(expanded_labels) > self.max_len:\n",
        "            expanded_labels = expanded_labels[:self.max_len]\n",
        "\n",
        "        enc['labels'] = expanded_labels\n",
        "\n",
        "        return enc\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # x = NERDataset(\n",
        "    #     file='data/ner/rahimi_output/eng/train',\n",
        "    #     max_len=256,\n",
        "    #     tokenizer=None,\n",
        "    #     assignment='last'\n",
        "    # )\n",
        "\n",
        "    inps = []\n",
        "    labels_found = []\n",
        "\n",
        "    with open('/content/drive/MyDrive/{lang}/train'.format(lang=finetune_lang)) as f:\n",
        "        temp_tokens = []\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                token = line.strip().split('\\t')\n",
        "                assert len(token) == 2\n",
        "                temp_tokens.append(\n",
        "                    (token[0].replace(finetune_lang + ':', ''), token[1])\n",
        "                )\n",
        "            else:\n",
        "                inps.append(temp_tokens)\n",
        "                temp_tokens = []\n",
        "\n",
        "    print(inps[5])\n",
        "    print(len(inps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2Pd1dxqzhg95"
      },
      "outputs": [],
      "source": [
        "# filefinder\n",
        "def biblelang2nerlang(bible_lang):\n",
        "    language_mapping = '/content/drive/MyDrive/NER/bible_ner_xlmr_split.txt'\n",
        "    with open(language_mapping, 'r') as f:\n",
        "        for line in f:\n",
        "            data = line.strip().split(',')\n",
        "            if data[1] == bible_lang:\n",
        "                return data[2]\n",
        "\n",
        "def lang_to_ner(lang, split):\n",
        "\n",
        "    ner_dir = '/content/drive/MyDrive/{lang}/{split}'.format(lang=lang, split=split)\n",
        "\n",
        "    return ner_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ppVrnon_frvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb486eed-9a47-47ef-af49-87abccde47e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['دايو', '(', 'شركة', ')'], [0, 1, 1, 1])\n",
            "----------------------------------------\n",
            "(['انتاجه', 'فى', 'امريكا', 'كندا', 'وبيتسعر', 'غالبن', 'فى', 'الصين', '.'], [8, 8, 6, 6, 8, 8, 8, 6, 8])\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# define training and evaluation dataset\n",
        "ner_train_dataset = NERDataset(file=lang_to_ner(finetune_lang, 'train'),\n",
        "                                       lang=finetune_lang, max_len=256, tokenizer=xlmr_tokenizer,\n",
        "                                       assignment='last')\n",
        "\n",
        "\n",
        "ner_eval_dataset = NERDataset(file=lang_to_ner(language, 'dev'),\n",
        "                          lang=biblelang2nerlang(language), max_len=256, tokenizer=xlmr_tokenizer,\n",
        "                          assignment='last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MFEXDuqYfsOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d645f83-b474-45a9-bbe5-242970a022fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['تحويل', 'احمد', 'بن', 'طولون'], [8, 2, 3, 3])\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ner_test_dataset = NERDataset(file=lang_to_ner(language, 'test'), lang=biblelang2nerlang(language), max_len=256, tokenizer=xlmr_tokenizer, assignment='last')\n",
        "\n",
        "\n",
        "ner_warmup_steps = int((5 * (ner_train_dataset.__len__() // (32 * 4 * 1))) * .01)\n",
        "!mkdir {finetune_lang}_finetuned_ner_model\n",
        "ner_training_args = TrainingArguments(\n",
        "    output_dir= '{finetune_lang}_finetuned_ner_model'.format(finetune_lang=finetune_lang),\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_steps=25,\n",
        "    save_total_limit=3,\n",
        "    save_steps=3000,\n",
        "    evaluation_strategy='epoch',\n",
        "    eval_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=ner_warmup_steps,\n",
        "    disable_tqdm=False,\n",
        "    gradient_accumulation_steps=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "H4Ao4qJzNB5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f301a45-05e9-46ec-8330-8ca987d23b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/arz_model/final_model were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /content/drive/MyDrive/arz_model/final_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(281922, 384, padding_idx=0)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/{lang}_model/final_model'.format(lang=language),num_labels=len(ner_train_dataset.label2id))\n",
        "\n",
        "ner_model.resize_token_embeddings(len(xlmr_tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ha8cfoP5pKBG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from seqeval.metrics import f1_score as seqeval_f1\n",
        "from seqeval.metrics import accuracy_score as seqeval_accuracy\n",
        "\n",
        "\n",
        "def create_id2label_ner():\n",
        "\n",
        "    ner_tags = [\n",
        "        'B-ORG',\n",
        "        'I-ORG',\n",
        "        'B-PER',\n",
        "        'I-PER',\n",
        "        'B-MISC',\n",
        "        'I-MISC',\n",
        "        'B-LOC',\n",
        "        'I-LOC',\n",
        "        'O'\n",
        "    ]\n",
        "\n",
        "    iter = 0\n",
        "    id2label = {}\n",
        "    for tag in ner_tags:\n",
        "        id2label[iter] = tag\n",
        "        iter += 1\n",
        "\n",
        "    return id2label\n",
        "\n",
        "\n",
        "\n",
        "def ner_metrics(eval_pred):\n",
        "\n",
        "    labels = eval_pred.label_ids\n",
        "    preds = eval_pred.predictions.argmax(-1)\n",
        "\n",
        "    corrected_preds = []\n",
        "    corrected_labels = []\n",
        "\n",
        "    id2label = create_id2label_ner()\n",
        "\n",
        "    for i in range(0, len(labels)):\n",
        "        temp_pred = []\n",
        "        temp_label = []\n",
        "        for j in range(0, len(labels[i])):\n",
        "            if labels[i][j] != -100:\n",
        "                temp_label.append(id2label[labels[i][j]])\n",
        "                temp_pred.append(id2label[preds[i][j]])\n",
        "\n",
        "        corrected_labels.append(temp_label)\n",
        "        corrected_preds.append(temp_pred)\n",
        "\n",
        "    acc = seqeval_accuracy(corrected_labels, corrected_preds)\n",
        "    f1 = seqeval_f1(corrected_labels, corrected_preds)\n",
        "\n",
        "    f1 = f1 * 100\n",
        "    acc = acc * 100\n",
        "\n",
        "    print('F1 during training: {}'.format(f1))\n",
        "    print('Accuracy during training: {}'.format(acc))\n",
        "    print('---------------------------------------------')\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Rov422qwoNmA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f8f1c74-91e0-429a-aefa-484925e37324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 20000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 3125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 20:43, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.806221</td>\n",
              "      <td>82.917214</td>\n",
              "      <td>54.263566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.514000</td>\n",
              "      <td>0.666207</td>\n",
              "      <td>84.756899</td>\n",
              "      <td>61.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.392600</td>\n",
              "      <td>0.540101</td>\n",
              "      <td>86.859396</td>\n",
              "      <td>62.835249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.336500</td>\n",
              "      <td>0.545243</td>\n",
              "      <td>86.465177</td>\n",
              "      <td>66.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.309800</td>\n",
              "      <td>0.530828</td>\n",
              "      <td>86.596583</td>\n",
              "      <td>66.409266</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 54.263565891472865\n",
            "Accuracy during training: 82.91721419185282\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 61.363636363636374\n",
            "Accuracy during training: 84.7568988173456\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 62.835249042145605\n",
            "Accuracy during training: 86.85939553219448\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 66.66666666666667\n",
            "Accuracy during training: 86.46517739816032\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ar_finetuned_ner_model/checkpoint-3000\n",
            "Configuration saved in ar_finetuned_ner_model/checkpoint-3000/config.json\n",
            "Model weights saved in ar_finetuned_ner_model/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 66.40926640926641\n",
            "Accuracy during training: 86.59658344283837\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 during training: 68.85245901639344\n",
            "Accuracy during training: 87.32970027247956\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ner_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=xlmr_tokenizer,\n",
        "    padding='longest'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=ner_model,\n",
        "    data_collator=ner_collator,\n",
        "    args=ner_training_args,\n",
        "    train_dataset=ner_train_dataset,\n",
        "    eval_dataset=ner_eval_dataset,\n",
        "    compute_metrics=ner_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.predict(ner_test_dataset)\n",
        "results = results.metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4OEDoBmRrq6",
        "outputId": "81cda797-aa7b-4d67-9e7e-65c8e88abe05"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_accuracy': 87.32970027247956,\n",
              " 'test_f1': 68.85245901639344,\n",
              " 'test_loss': 0.5435361862182617,\n",
              " 'test_runtime': 0.4363,\n",
              " 'test_samples_per_second': 229.191,\n",
              " 'test_steps_per_second': 29.795}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MiniLM_extend_ner_diffFinetuneLang",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WancTPIprpkwiCoblUw7Q8rP1LmwqnLP",
      "authorship_tag": "ABX9TyOBtQwNW4Y4XdjMBSXp2QnX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}